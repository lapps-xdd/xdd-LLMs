# LLMs for AskMe and xDD

Using Large Language Models for xDD processing.

This includes running summarization and entity extraction, using GPT and Llama.

## Requirements

We have used this on Python 3.10.12 and 3.11.6. Python modules needed:

```shell
pip install langchain==0.1.17
pip install openai==1.35.2
pip install tqdm
```

Download Ollama from [https://github.com/ollama/ollama](https://github.com/ollama/ollama) and install it. To installing on Linux:

```shell
curl -fsSL https://ollama.com/install.sh | sh
```

Then run ollama once so that the model is installed (we are using llama2 here):

```bash
$ ollama run llama2
```

This first loads the model (which is around 4GB) then starts the chat. Play with the chat if you like, then exit.

On Linux, models are stored in `/usr/share/ollama/.ollama/models`, on Mac OSX, they are in `~/.ollama/models` (see the [faq](https://github.com/ollama/ollama/blob/main/docs/faq.md)).

If using OpenAI, you need a token. Either set an environment variable in your shell (see [https://help.openai.com](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)) or edit `run_llm/run_gpt`:

```python
client = OpenAI(api_key="SOME_API_KEY")
```


## Running Instructions

Put your data in the `data/` folder structured as below:

```
data/
├── biomedical
│   └── output
│       └── doc
├── mars
│   └── output
│       └── doc
└── random
    └── output
        └── doc
```

That is, under the top-level directory we have a directory for each domain (or document set, or datadrop) which itself has a `output/doc` subdirectory.

Input data are created using the xDD document processing code in [https://github.com/lapps-xdd/xdd-docstructure](https://github.com/lapps-xdd/xdd-docstructure), but the directory structure is the one generated by the code in [https://github.com/lapps-xdd/xdd-integration](https://github.com/lapps-xdd/xdd-integration), which calls the document parser.


### Summarization and Entity Extraction with GPT

Run the code in the `run_llm/run_gpt.py` script.

```shell
python -m run_llm.run_gpt
```

You need an [OpenAI API key](https://openai.com/) to run this code.

You can switch between GPT-3.5 and GPT-4 by configuring the parameters near the top of the file:

```python
COMPLETION_PARAMS = {
    "model": "gpt-4",  # or "gpt-3.5-turbo"
    "temperature": 0,
    "max_tokens": 500,  # max tokens in the output
    "messages": [],
}
```


### Summarization and Entity Extraction with LLama2

```shell
python -m run_llm.run_ollama
```

You need to download a LLama2 checkpoint using [Ollama](https://github.com/ollama/ollama) to run this code.
