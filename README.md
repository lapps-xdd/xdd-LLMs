# LLMs for AskMe and xDD

Using Large Language Models for xDD processing.

This includes running summarization and entity extraction, using GPT and Llama. For now this is mostly focused on using Llama3 to extract summaries.


## Requirements

This will run on a recent CPU with some power, but it will be breathtakingly slow (on a recent Mac powerbook with an m1 chip it took 1-2 minutes per document to get a summary out of Llama). We typically run this on a 10GB GPU, but it should run on a 8GB. See the Docker section below for some extra notes on GPUs and Docker.

To get this code if you do not already have it:

```shell
git clone https://github.com/lapps-xdd/xdd-LLMs
```

We have used this on Python 3.10.12 and 3.11.6. Python modules needed:

```shell
pip install langchain==0.1.17
pip install openai==1.35.2
```

Download Ollama from [https://github.com/ollama/ollama](https://github.com/ollama/ollama) and install it. To install on Linux:

```shell
curl -fsSL https://ollama.com/install.sh | sh
```

To pull the Llama3 model:

```shell
ollama pull llama3
```

On Linux, models are stored in `/usr/share/ollama/.ollama/models`, see the [faq](https://github.com/ollama/ollama/blob/main/docs/faq.md)).

If using OpenAI's GPT, you need an [OpenAI API key](https://openai.com/) to run the code. To use it either set an environment variable in your shell (see [https://help.openai.com](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)) or edit `llm/run_gpt`:

```python
client = OpenAI(api_key="SOME_API_KEY")
```

If you choose not to use and install OpenAI, then you should install the tqdm module.

```shell
pip install tqdm
```


## Running the code


### Summarization and Entity Extraction with Llama

First make sure that Ollama is running with one of the following commands (the first one is prefered, but use the second if the first ain't working):

```shell
systemctl start ollama
```

```shell
ollama start &
```

To run the summarizer do the following:

```shell
cd code
python -m llm.run_ollama --doc INDIR --sum OUTDIR [--overwrite] [--limit N]
```

This processes the files in INDIR and output is written to individual text files in OUTDIR. The --limit option limits processing to N files. If a file already exists in OUTDIR it won't be overwritten unless the --overwrite option is used.

Input data are expected to have been created by the xDD document processing code in [https://github.com/lapps-xdd/xdd-docstructure](https://github.com/lapps-xdd/xdd-docstructure)

The code uses Llama3 by default, if you want to use Llama2 edit the following line in `llm/run_ollama`:

```python
llm = Ollama(model="llama3", temperature=0, callback_manager=None)
```

The llama2 model is slightly smaller, 4.8GB versus 4.7GB, and seems to run a little bit slower, both take around 2s per document for summarization. If you use Llam


#### Legacy code to run both the summarizer and the entity extraction

You can also forgo any of the options and just do

```shell
python -m llm.run_ollama
```

In that case the code expects there to be a directory `code/data/` structured as below:

```
data/
├── biomedical
│   └── output
│       └── doc
├── mars
│   └── output
│       └── doc
└── random
    └── output
        └── doc
```

That is, under the top-level directory we have a directory for each domain (or document set, or datadrop) which itself has a `output/doc` subdirectory.

Input data are still created using the xDD document processing code in [https://github.com/lapps-xdd/xdd-docstructure](https://github.com/lapps-xdd/xdd-docstructure), but the directory structure is the one generated by the code in [https://github.com/lapps-xdd/xdd-integration](https://github.com/lapps-xdd/xdd-integration), which calls the document parser.

In this mode the output will be in a couple of files in the working directory, the filenames will match `TOPIC_ollama_7b_entity.jsonl` or `TOPIC_ollama_7b_summarization.jsonl`.


### Summarization and Entity Extraction with GPT

Run the code in the `llm/run_gpt.py` script.

```shell
python -m llm.run_gpt
```



You can switch between GPT-3.5 and GPT-4 by configuring the parameters near the top of the file:

```python
COMPLETION_PARAMS = {
    "model": "gpt-4",  # or "gpt-3.5-turbo"
    "temperature": 0,
    "max_tokens": 500,  # max tokens in the output
    "messages": [],
}
```


## Docker

### Requirements

To run a Docker container and have it use the GPU on the host there are some requirements.

Following instructions at [https://www.howtogeek.com/devops/how-to-use-an-nvidia-gpu-with-docker-containers/](https://www.howtogeek.com/devops/how-to-use-an-nvidia-gpu-with-docker-containers/).

First your Docker host needs to have a GPU. Test this with the `nvidia-smi` command.

Then you want to add the  NVIDIA Container Toolkit to your host, see [https://github.com/NVIDIA/nvidia-docker](https://github.com/NVIDIA/nvidia-docker). First step there is to get the package repository


```shell
distribution=$(. /etc/os-release;echo $ID$VERSION_ID) && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
```

This does give a depreciation warning for apt-key, but it does seem to succeed.

Next install the nvidia-docker2 package on your host:

```shell
sudo apt-get update   
sudo apt-get install -y nvidia-docker2
```

Restart the Docker daemon to complete the installation:

```shell
sudo systemctl restart docker
```

> Note that while the above still works it is outdated since nvidia-docker2 is now deprecated. Instead use what is at [https://github.com/NVIDIA/nvidia-container-toolkit](https://github.com/NVIDIA/nvidia-container-toolkit)


### Creating the image

To build the image do this:

```shell
cd code
docker build -t xdd-llms .
```

This can take a while (up to 5 minutes) because it has a fair amount of software to install and a 4.7GB model to download.


### Running the container

Start the container with the --gpus flag and a dorectory mount:

```shell
docker run --rm -it --gpus all -v /host_path_to_data:/data xdd-llms bash
```

We assume for this example that `host_path_to_data` has a `doc` sub directory with input files.
 
Once in the container you first need to start Ollama:

```shell
root@06f6bc4aa555:/app# ollama start &
```

Then you can run the summarizer:

```shell
root@06f6bc4aa555:/app# python3 -m llm.run_ollama --doc /data/doc/ --sum /data/sum
```

